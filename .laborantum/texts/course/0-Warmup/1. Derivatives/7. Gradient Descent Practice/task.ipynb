{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Gradient Descent Practice"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Gradient Descent is the core numerical optimization technique that is used in Machine Learning. In this practice we are going to code the Gradient Descent Algorithm and use it on 1D and 2D functions.\n",
                "\n",
                "Exactly the same way it works with higher dimensional functions with the only exception that it is impossible to visualize the process."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import json_tricks\n",
                "\n",
                "answer = {}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 1. Code Gradient Descent algorith."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The inputs into your function are:\n",
                "- current position $\\mathbf x$, `x`\n",
                "- gradient of the objective function $\\nabla L$, `grad`\n",
                "- learning rate $\\alpha$\n",
                "\n",
                "Code the update step of Gradient Descent algorithm."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "def grad_descent(x, grad, alpha: float = 0.1):\n",
                "\n",
                "    return x - alpha * grad"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 2. Code Objective Function"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "We will use a super simple objective function:\n",
                "\n",
                "$f(x) = x^2$\n",
                "\n",
                "Code this function and its gradient below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "# 1D objective: f(x) = x^2\n",
                "def f_1d(x):\n",
                "    x = np.asarray(x)\n",
                "    return x**2\n",
                "\n",
                "def grad_f_1d(x):\n",
                "    x = np.asarray(x)\n",
                "    return 2 * x"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [],
            "source": [
                "position = 5\n",
                "\n",
                "history = []\n",
                "for index in range(10):\n",
                "    grad = grad_f_1d(position)\n",
                "    position = grad_descent(position, grad)\n",
                "    history.append(position)\n",
                "\n",
                "xs = np.linspace(-6, 6, 100)\n",
                "ys = f_1d(xs)\n",
                "\n",
                "history = np.array(history)\n",
                "\n",
                "plt.plot(xs, f_1d(xs), label=\"Loss function\")\n",
                "plt.plot(history, f_1d(history), 'o', label=\"Optimization path\")\n",
                "dx = np.diff(history)  # Change in x\n",
                "dy = np.diff(f_1d(history))       # Change in y\n",
                "plt.quiver(history[:-1], f_1d(history)[:-1], dx, dy, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"red\", label=\"Steps\")\n",
                "\n",
                "plt.legend()\n",
                "plt.title(\"Optimization Path with Arrows\")\n",
                "plt.show()\n",
                "\n",
                "answer['1d'] = history.tolist()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Task 3. 2D optimization\n",
                "\n",
                "The simple test case is passed. Let us take a look at a 2D function and how is it going to be optimized.\n",
                "\n",
                "In this case we will be optimizing a slightly more sophisticated function:\n",
                "\n",
                "$f(x, y) = (1 - x)^2 + 10 (y - x^2)^2$\n",
                "\n",
                "Note that in this case, the input in loss function is a 2D point with coordinates:\n",
                "\n",
                "```\n",
                "numpy.array([1, 2])\n",
                "```\n",
                "\n",
                "Write a function that calculates the objective function and its gradient"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "\n",
                "# f(x, y) = (1 - x)^2 + 10 * (y - x^2)^2\n",
                "def loss_2d(v):\n",
                "    v = np.asarray(v, dtype=float)\n",
                "    # Support both shapes: (2, ...) and (..., 2)\n",
                "    if v.ndim >= 2 and v.shape[0] == 2:\n",
                "        x, y = v[0], v[1]            # stacked along first axis\n",
                "    else:\n",
                "        x, y = v[..., 0], v[..., 1]  # last-dimension pair\n",
                "    return (1 - x)**2 + 10 * (y - x**2)**2\n",
                "\n",
                "def loss_2d_grad(v):\n",
                "    v = np.asarray(v, dtype=float)\n",
                "    if v.ndim >= 2 and v.shape[0] == 2:\n",
                "        x, y = v[0], v[1]\n",
                "        dfx = 2 * (x - 1) - 40 * x * (y - x**2)\n",
                "        dfy = 20 * (y - x**2)\n",
                "        return np.stack([dfx, dfy], axis=0)   # matches (2, ...) layout\n",
                "    else:\n",
                "        x, y = v[..., 0], v[..., 1]\n",
                "        dfx = 2 * (x - 1) - 40 * x * (y - x**2)\n",
                "        dfy = 20 * (y - x**2)\n",
                "        return np.stack([dfx, dfy], axis=-1)  # matches (..., 2) layout"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [],
            "source": [
                "position = np.array([-1, 0])\n",
                "\n",
                "history = [position]\n",
                "for index in range(1000):\n",
                "    grad = loss_2d_grad(position)\n",
                "    position = grad_descent(position, grad, alpha=0.02)\n",
                "    history.append(position)\n",
                "\n",
                "xs = np.linspace(-2, 2, 400)\n",
                "ys = np.linspace(-1, 3, 400)\n",
                "\n",
                "xs, ys = np.meshgrid(xs, ys)\n",
                "\n",
                "history = np.array(history)\n",
                "\n",
                "plt.contour(xs, ys, loss_2d(np.stack([xs, ys])), label=\"Loss function\", levels=np.logspace(0, 3, 20))\n",
                "x, y = history[:, 0], history[:, 1]\n",
                "dx = np.diff(x)\n",
                "dy = np.diff(y)\n",
                "plt.quiver(x[:-1], y[:-1], dx, dy, angles=\"xy\", scale_units=\"xy\", scale=1, color=\"red\", width=0.005)\n",
                "plt.plot(x, y, 'o', label=\"Optimization path\")\n",
                "\n",
                "answer['2d'] = history.tolist()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Afterword\n",
                "\n",
                "That is really impressive! We have coded one of the two main algorithms in Deep Learning: a Gradient Descent!\n",
                "\n",
                "- How does the gradient descent work? Is its behavior logical?\n",
                "- Which of the steps can potentially be dangerous?\n",
                "- How would you improve Gradient Descent so that there is no that unwanted dangerous step?\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [],
            "source": [
                "json_tricks.dump(answer, '.answer.json')"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
